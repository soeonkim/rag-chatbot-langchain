import gradio as gr
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

load_dotenv()

# ë°ì´í„° í´ë” ê²½ë¡œ
DATA_DIR = "./data"

def get_llm():
    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0,
        max_tokens=None,
        timeout=None,
        max_retries=2,
        # other params...
    )

# ì „ì—­ ë³€ìˆ˜: Lazy Loadingì„ ìœ„í•œ ìºì‹œ
_embeddings_cache = None
_retriever_cache = None

def get_embeddings():
    """
    ì„ë² ë”© ëª¨ë¸ì„ Lazy Loadingìœ¼ë¡œ ë°˜í™˜
    - ìµœì´ˆ í˜¸ì¶œ ì‹œì—ë§Œ ì´ˆê¸°í™”
    - ì´í›„ í˜¸ì¶œ ì‹œ ìºì‹œëœ ê°ì²´ ì¬ì‚¬ìš©
    """
    global _embeddings_cache

    if _embeddings_cache is not None:
        print("âœ… ìºì‹œëœ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©")
        return _embeddings_cache

    print("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ìµœì´ˆ ë¡œë”© ì¤‘...")
    _embeddings_cache = HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-large"
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ë° ìºì‹±")
    return _embeddings_cache

def get_retriever():
    """
    Retrieverë¥¼ Lazy Loadingìœ¼ë¡œ ë°˜í™˜
    - ìµœì´ˆ í˜¸ì¶œ ì‹œì—ë§Œ ì´ˆê¸°í™” (KB ë¯¸ì‚¬ìš© ì‹œ ë¦¬ì†ŒìŠ¤ ë‚­ë¹„ ì—†ìŒ)
    - ì´í›„ í˜¸ì¶œ ì‹œ ìºì‹œëœ ê°ì²´ ì¬ì‚¬ìš© (ë¹ ë¥¸ ì‘ë‹µ)
    """
    global _retriever_cache

    # ì´ë¯¸ ì´ˆê¸°í™”ëœ ê²½ìš° ìºì‹œëœ ê°ì²´ ë°˜í™˜
    if _retriever_cache is not None:
        print("âœ… ìºì‹œëœ Retriever ì‚¬ìš©")
        return _retriever_cache

    # ìµœì´ˆ í˜¸ì¶œ ì‹œì—ë§Œ ì´ˆê¸°í™”
    try:
        print("ğŸ”§ Retriever ìµœì´ˆ ì´ˆê¸°í™” ì¤‘...")
        embeddings = get_embeddings()  # ê³µí†µ í•¨ìˆ˜ ì‚¬ìš©

        vector_store = Chroma(
            collection_name="example_collection",
            embedding_function=embeddings,
            persist_directory="./chroma_langchain_db"
        )

        # ë²¡í„°DBê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        try:
            doc_count = vector_store._collection.count()
            if doc_count == 0:
                print("âš ï¸ ë²¡í„°DBê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. 'ë²¡í„°DB ë¦¬ë¡œë“œ'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
                return None
            print(f"ğŸ“Š ë²¡í„°DBì— {doc_count}ê°œ ë¬¸ì„œ ì¡´ì¬")
        except Exception as count_error:
            print(f"âš ï¸ ë¬¸ì„œ ê°œìˆ˜ í™•ì¸ ì‹¤íŒ¨: {count_error}")
            return None

        _retriever_cache = vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5},
        )
        print("âœ… Retriever ì´ˆê¸°í™” ì™„ë£Œ ë° ìºì‹±")
        return _retriever_cache

    except Exception as e:
        print(f"âŒ Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None
    
llm = get_llm()

def format_docs(docs):
    """
    Chromaì—ì„œ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜

    Args:
        docs: LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸

    Returns:
        str: í¬ë§·ëœ ë¬¸ì„œ ë‚´ìš©
    """
    if not docs:
        print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ")
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    formatted = []
    for idx, doc in enumerate(docs, 1):
        # ChromaëŠ” LangChain Document ê°ì²´ë¥¼ ë°˜í™˜
        content = doc.page_content.strip()

        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶œì²˜ ì •ë³´ ì¶”ì¶œ
        source = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
        page = doc.metadata.get("page")

        # ë¬¸ì„œ ë²ˆí˜¸ì™€ ì¶œì²˜ ì •ë³´ í¬í•¨
        doc_info = f"[ë¬¸ì„œ {idx}] ì¶œì²˜: {source}"
        if page is not None:
            # pageëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1
            doc_info += f", í˜ì´ì§€: {page + 1}"

        formatted.append(f"{doc_info}\n{content}")

    result = "\n\n---\n\n".join(formatted)
    print(f"âœ… {len(formatted)}ê°œ ë¬¸ì„œ í¬ë§· ì™„ë£Œ (ì´ {len(result)}ì)")
    return result

def create_chain_with_kb(retriever):
    """RAG ì²´ì¸ ìƒì„± - Retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰ í›„ LLMì— ì „ë‹¬"""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ë§¥ì— ë‹µì´ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì–´ëŠ ë¬¸ì„œì˜ ì–´ëŠ í˜ì´ì§€ë¥¼ ì°¸ê³ í–ˆëŠ”ì§€ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
ì˜ˆì‹œ: "[ì¶œì²˜: manual.pdf 3í˜ì´ì§€]"

Context:
{context}
""",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    def retrieve_and_format(x):
        """ê²€ìƒ‰ ì‹¤í–‰ ë° í¬ë§·íŒ…"""
        try:
            input_text = x["input"] if isinstance(x, dict) else x
            print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{input_text}'")

            retrieved_docs = retriever.invoke(input_text)
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(retrieved_docs) if retrieved_docs else 0}ê°œ")

            return format_docs(retrieved_docs)
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    # ì²´ì¸ êµ¬ì„±: ê²€ìƒ‰ â†’ í”„ë¡¬í”„íŠ¸ â†’ LLM
    return (
        {
            "context": retrieve_and_format,
            "chat_history": lambda x: x["chat_history"],
            "input": lambda x: x["input"],
        }
        | prompt
        | llm
    )


def create_chain_without_kb():
    """ì¼ë°˜ ëŒ€í™”ìš© ì²´ì¸ - KB ì—†ì´ LLMë§Œ ì‚¬ìš©"""
    prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    return prompt | llm

def load_and_chunk_pdfs():
    """
    data í´ë”ì˜ ëª¨ë“  PDFë¥¼ ë¡œë“œí•˜ê³  ì²­í¬ë¡œ ë¶„í• 

    Returns:
        tuple: (chunks, status_message)
    """
    try:
        # data í´ë”ì˜ ëª¨ë“  PDF íŒŒì¼ ì°¾ê¸°
        pdf_files = list(Path(DATA_DIR).glob("*.pdf"))

        if not pdf_files:
            return [], "âŒ data í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

        all_chunks = []

        # ê° PDF íŒŒì¼ ì²˜ë¦¬
        for pdf_path in pdf_files:
            print(f"ğŸ“„ ë¡œë”© ì¤‘: {pdf_path.name}")

            # PDF ë¡œë“œ
            loader = PyPDFLoader(str(pdf_path))
            docs = loader.load()

            # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, chunk_overlap=200, add_start_index=True
            )
            
            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            chunks = text_splitter.split_documents(docs)

            # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ëª… ì¶”ê°€
            for chunk in chunks:
                chunk.metadata["source"] = pdf_path.name

            all_chunks.extend(chunks)
            print(f"âœ… {pdf_path.name}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        status_msg = f"âœ… ì„±ê³µ!\n- PDF íŒŒì¼ ìˆ˜: {len(pdf_files)}\n- ì´ ì²­í¬ ìˆ˜: {len(all_chunks)}"
        return all_chunks, status_msg

    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return [], error_msg


def save_to_vectorstore(chunks, embeddings):
    """
    ì²­í¬ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì €ì¥)

    Args:
        chunks (list): ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        embeddings: ì„ë² ë”© ëª¨ë¸

    Returns:
        tuple: (vector_store, ids)
    """

    print("ğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ ì¤‘...")
    vector_store = Chroma(
        collection_name="example_collection",
        embedding_function=embeddings,
        persist_directory="./chroma_langchain_db"
    )

    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (íŒŒì¼ì€ ìœ ì§€)
    try:
        existing_ids = vector_store.get()["ids"]
        if existing_ids:
            print(f"ğŸ—‘ï¸  ê¸°ì¡´ ë°ì´í„° {len(existing_ids)}ê°œ ì‚­ì œ ì¤‘...")
            vector_store.delete(ids=existing_ids)
            print("âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸  ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ì§„í–‰): {e}")

    # ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    ids = vector_store.add_documents(documents=chunks)

    print(f"âœ… {len(ids)}ê°œ ë¬¸ì„œë¥¼ ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥ ì™„ë£Œ")
    return vector_store, ids

def reload_vectorstore():
    """
    ë²¡í„°ìŠ¤í† ì–´ ë¦¬ë¡œë“œ (PDF ë¡œë“œ â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥)

    Returns:
        str: ì‹¤í–‰ ê²°ê³¼ ë©”ì‹œì§€
    """
    global _retriever_cache

    try:
        # 1ë‹¨ê³„: PDF ë¡œë“œ ë° ì²­í‚¹
        chunks, status = load_and_chunk_pdfs()

        if not chunks:
            return status

        # 2ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ì‚¬ìš©)
        embeddings = get_embeddings()

        # 3ë‹¨ê³„: ë²¡í„°ìŠ¤í† ì–´ì— ì €ì¥
        vector_store, ids = save_to_vectorstore(chunks, embeddings)

        # 4ë‹¨ê³„: Retriever ìºì‹œ ë¬´íš¨í™” (ìƒˆë¡œìš´ ë°ì´í„° ë°˜ì˜)
        _retriever_cache = None
        print("ğŸ”„ Retriever ìºì‹œ ì´ˆê¸°í™” (ë‹¤ìŒ ê²€ìƒ‰ ì‹œ ìƒˆ ë°ì´í„° ë°˜ì˜)")

        # ì„±ê³µ ë©”ì‹œì§€
        result = f"""âœ… Knowledge Base ë¦¬ë¡œë“œ ì™„ë£Œ!

ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:
{status}
- ì €ì¥ëœ ë¬¸ì„œ ID ìˆ˜: {len(ids)}
- ì €ì¥ ìœ„ì¹˜: ./chroma_langchain_db
"""
        return result

    except Exception as e:
        error_msg = f"âŒ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{str(e)}"
        print(error_msg)
        return error_msg

# ì „ì—­ ë³€ìˆ˜: RAG ì‚¬ìš© ì—¬ë¶€
use_rag = False

def chatbot(message, history):
    """
    ì±—ë´‡ ì‘ë‹µ ìƒì„± (RAG ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°)

    Args:
        message (str): ì‚¬ìš©ì ì…ë ¥
        history (list): ëŒ€í™” íˆìŠ¤í† ë¦¬

    Yields:
        str: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
    """
    # LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    langchain_messages = []

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ LangChain í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    for msg in history:
        if msg["role"] == "user":
            langchain_messages.append(HumanMessage(content=msg["content"]))
        else:
            langchain_messages.append(AIMessage(content=msg["content"]))

    # RAG ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì²´ì¸ ì„ íƒ
    if use_rag:
        retriever = get_retriever()
        if retriever is None:
            # ë²¡í„°DBê°€ ë¹„ì–´ìˆê±°ë‚˜ ì´ˆê¸°í™” ì‹¤íŒ¨
            error_msg = """âš ï¸ RAG ëª¨ë“œê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ë§Œ Knowledge Baseë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
1. data/ í´ë”ì— PDF íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
2. ìš°ì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ 'Knowledge Base Sync' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
3. ë¦¬ë¡œë“œ ì™„ë£Œ í›„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"""
            yield error_msg
            return
        chain = create_chain_with_kb(retriever)
    else:
        chain = create_chain_without_kb()

    # ì²´ì¸ ì…ë ¥ ì¤€ë¹„
    chain_input = {
        "chat_history": langchain_messages,
        "input": message,
    }

    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
    full_response = ""
    for chunk in chain.stream(chain_input):
        for char in chunk.content:
            full_response += char
            yield full_response + "â–Œ"

    # ë§ˆì§€ë§‰: ì»¤ì„œ ì œê±°
    yield full_response

def toggle_rag(use_rag_mode):
    """
    RAG ëª¨ë“œ í† ê¸€

    Args:
        use_rag_mode (bool): RAG ì‚¬ìš© ì—¬ë¶€
    """
    global use_rag
    use_rag = use_rag_mode
    print(f"ğŸ”„ RAG ëª¨ë“œ: {'í™œì„±í™”' if use_rag else 'ë¹„í™œì„±í™”'}")

with gr.Blocks() as demo:
    gr.Markdown("# ğŸ“š LangChainì„ í™œìš©í•œ RAG ê¸°ë°˜ ì±—ë´‡")

    with gr.Row():
        with gr.Column(scale=3):
            gr.ChatInterface(
                fn=chatbot,
                type="messages"
            )

        with gr.Column(scale=1):
            gr.Markdown("### âš™ï¸ ì„¤ì •")

            # RAG ëª¨ë“œ í† ê¸€
            rag_toggle = gr.Checkbox(
                label="RAG ëª¨ë“œ ì‚¬ìš©",
                value=False,
                info=" Knowledge Baseë¥¼ í™œìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€"
            )

            # RAG í† ê¸€ ì´ë²¤íŠ¸
            rag_toggle.change(
                fn=toggle_rag,
                inputs=rag_toggle
            )

            gr.Markdown("---")
            gr.Markdown("### ğŸ“¦ Knowledge Base ê´€ë¦¬")

            reload_btn = gr.Button("ğŸ”„ Knowledge Base Sync", variant="primary")
            status_output = gr.Textbox(
                label="ìƒíƒœ",
                lines=5,
                interactive=False
            )

            # ë²„íŠ¼ í´ë¦­ ì´ë²¤íŠ¸
            reload_btn.click(
                fn=reload_vectorstore,
                outputs=status_output
            )

            gr.Markdown("""
            **ì‚¬ìš© ë°©ë²•:**
            1. `data/` í´ë”ì— PDF íŒŒì¼ ì¶”ê°€
            2. 'Knowledge Base Sync' ë²„íŠ¼ í´ë¦­
            3. 'RAG ëª¨ë“œ ì‚¬ìš©' ì²´í¬
            4. ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ì‹œì‘
            """)

if __name__ == "__main__":
    demo.launch()